=== 第1章_关系数据库查询优化.pptx ===
第1章 关系数据库查询优化


《智能数据工程》

清华大学出版社
2025年1月
提纲
关系数据库查询优化概述
基数估计
基数估计概述
传统基数估计方法
基于机器学习的基数估计方法
总结

关系数据库查询优化概述 (1)
数据管理（Data Management）技术
信息系统开发和建设的核心技术
大数据和人工智能研究及应用的重要基础

数据库（Database）
需求分析：数据、功能、性能等需求
概念结构设计：设计E-R图
逻辑结构设计：设计优化关系模式
物理结构设计：设计系统架构（如存储结构、系统配置等）
数据库实施：编程实现并测试数据库程序


数据管理技术的具体实现
关系数据库查询优化概述 (2)
查询优化（Query Optimization）
对查询语句、索引设计、查询执行计划等方面进行调整优化
提高系统响应速度、可伸缩性和可用性
数据库物理设计的重要任务
查询优化常见手段
使用索引（Index）：用预处理的时空开销换取高效查询
反规范化（De-normalization）：减少连接操作次数和磁盘I/O
物化视图（Materialized View）：预存视图
查询重写（Query Rewriting）：重写语句获得更好的查询执行计划
优化目标——减少查询所需时间和资源消耗
提纲
关系数据库查询优化概述
基数估计
基数估计概述
传统基数估计方法
基于机器学习的基数估计方法
总结

基数估计概述 (1)
查询优化器（Query Optimizer）
从查询操作的所有执行计划（通过应用查询优化各种手段可得到的查询执行计划）中尽可能找到最优的计划


估计执行计划的代价（I/O成本、CPU成本）
估计查询涉及数据对象的大小分布情况（查询结果大小、可能取值频率分布）
基数估计概述 (2)
基数估计（Cardinality Estimation）
     估计数据中满足查询条件的记录数
选择性估计（Selectivity Estimation）
     估计数据中满足查询条件的概率值
基数估计和规模分布估计器有什么关系？

等价
基数估计方法
传统基数估计方法
基于机器学习的基数估计方法
当前数据库领域研究的热点！
提纲
关系数据库查询优化概述
基数估计
基数估计概述
传统基数估计方法
基于机器学习的基数估计方法
总结

传统基数估计方法 (1)
传统基数估计方法
基于概要的方法
预先收集数据库的统计信息，基于独立性等简单假设，快速计算基数值
代表性方法：基于直方图的方法




基于采样的方法
从原始数据中随机抽取一定比例或一定数量的记录，根据在采样集上执行查询得到的结果大小除以相应缩放比例得到基数估计结果
① 频率直方图（Frequency Histogram）
② 顶频直方图（Top Frequency Histogram）
③ 高度均衡直方图（Height Balanced Histogram）
④ 混合直方图（Hybrid Histogram）
根据不同数据分布情况
传统基数估计方法 (2)
基于直方图的基数估计方法
不同类型的直方图对应不同的基数估计方法，根据数据列的不同分布情况选择适合的直方图
存储桶数目（b）：收集统计信息时划分的块数
不同取值数目（Number Distinct Values, NDV）
估计百分比：目标列中用于统计分析的记录占总记录数的百分比
传统基数估计方法 (3)
传统基数估计方法 (4)
传统基数估计方法 (5)
顶频直方图
频率直方图的变体：仅选取了前b个目标列分量值
端点值与端点数含义和频率直方图相同
流行值：被选中的分量值
      非流行值：未被选中的分量值
传统基数估计方法 (6)
传统基数估计方法 (7)
基于混合直方图的基数估计
引入记录端点值重复次数的指标加速计算
流行值：对应存储桶端点值重复次数
非流行值：同高度均衡直方图
传统基数估计方法 (8)
基于采样的基数估计方法
使用采样的方法对查询数据进行缩放，通过对摘要信息进行连接和筛选得到采样基数值，再按缩放比例还原出原始数据的基数值

多表基数估计
单独对每个表进行采样，再将采样结果连接并进行查询并缩放
有什么弊端？
传统基数估计方法 (9)
提纲
关系数据库查询优化概述
基数估计
基数估计概述
传统基数估计方法
基于机器学习的基数估计方法
总结

基于机器学习的基数估计方法 (1)
基于机器学习的基数估计
查询驱动的方法
以历史查询记录为数据来源，使用回归模型（Regression）和神经网络（Neural Network），通过构建输入为查询、输出为实际基数的模型来实现基数估计
数据驱动的方法
以数据库中的原始数据记录作为主要数据来源，考虑选择性估计问题，使用概率图模型（Probabilistic Graph Model, PGM）和自回归模型（AutoRegressive Model, ARM）等构建列的联合分布模型，然后利用联合分布计算满足查询条件的概率
基于机器学习的基数估计 (2)

网络结构
基于机器学习的基数估计 (3)

基于机器学习的基数估计 (4)
基于机器学习的基数估计 (5)
基于机器学习的基数估计 (6)
基于机器学习的基数估计 (7)
第一个乘积项对应的神经网络输入
是什么？
例：Naru（Neural Relation Understanding）
将属性值转为特征向量的编码策略
属性取值个数较少时采用One-hot向量表示
将One-hot向量与可学习嵌入矩阵相乘，得到属性值的低维嵌入向量表示
基于机器学习的基数估计 (8)
基于自回归模型的基数估计
基于机器学习的基数估计 (9)
提纲
关系数据库查询优化概述
基数估计
基数估计概述
传统基数估计方法
基于机器学习的基数估计
总结

总结
关系数据库查询优化
常见的查询优化手段（使用索引，反规范化，物化视图，查询重写）
查询优化器：找出开销最小的最优查询执行计划
基数估计用于查询优化器的规模分布估计器
基数估计（估计满足查询条件的记录数）
传统基数估计方法（直方图，采样）
基于机器学习的基数估计
 查询驱动—回归模型、神经网络
 数据驱动—概率图模型、自回归模型
结语


谢谢！

=== 第2章_信息检索.pptx ===
第2章 信息检索
《智能数据工程》

清华大学出版社
2025年1月
提纲
信息检索概述
信息检索模型
文本信息检索
Web信息检索
信息检索评价指标
总结
信息检索概述 (1) 
典型的信息检索场景
  

用户
百度




信息检索（ Information Retrieval ）本质上是一种有目的和组织的信息存取活动，包含“存”和“取”两个基本环节
信息检索概述 (2)
用户的 提问


文本检索
数值检索
音频检索
文献检索、
事实检索等
数据检索、
数据处理等
音频检索、
视频检索等
提纲
信息检索概述
信息检索模型
文本信息检索
Web信息检索
信息检索评价指标
总结
信息检索系统的形式表示
表示为一个四元组(D, Q, F, R<dj, q>)，其中：
D为信息资源集合
Q为用户信息需求集合
F为D与Q的匹配处理框架
R<dj, q>为D与Q的相关性匹配函数
文本检索的一般流程
信息检索模型 (1)
索引项（关键词）
dj  (j=1, 2, …, n)表示一个文档，描述为一个集合，包含具有代表性的关键词（索引项）
词汇表
K = {k1, k2, …, kt}是文档集中所有不同索引项ki的集合，K为文档集的词汇表
向量空间模型（Vector Space Model, VSM）——部分匹配策略
文档向量：将D中的文档表示为文档向量集的形式
若词 kj 在文档 di 中出现 x 次，则文档 di 的向量在位置 j 上的值为x
di 的文档向量中某个位置的值为0，表示该词没有在该文档中出现 
 

    
词频的Zipf分布
   
信息检索模型 (2)
文档索引词集		             文档向量
只有极少数的词被经常使用，而绝大多数的词都很少使用。频度最高的词和频度最低的词所含有的信息量最少
一个词（非停止词）对于搜索而言（区分作用）
   ① 出现的次数越多越重要（一个文档）
   ② 包含该词的文档数越少越重要（多个文档的文档集）
体现这两个性质的词权重计算方法
 词权值的大小取决于以下两个方面的因素（dj中的词ki的权值）：
  ① 局部权值（词频）  fij=freqij/max tfj —第i个词在第j个文档中的权值
  ② 全局权值（文档频率）idfi=log(n/ni)—第i个词在整个文档集中的权值
 tf-idf加权模式——wij=fij*idfi






信息检索模型 (3)
 - n—系统中文档总数； ni—系统中含有ki的文档数
  - freqij—ki在dj中出现的次数
  - idfi—ki的逆文档频率（也称倒排文档频率）
  - max tfj—dj中所有词出现次数的最大值（消除文档长度对词权的影响） 
信息检索模型 (4)
【例】文档总数n=4，系统中含有索引词k1(agent) 的文档数n1=2
索引词k1在文档d1中出现的次数freq11=2
k1的逆文档频率idf1=log(n/n1)=log2
文档d1中所有索引词出现次数的最大值max tf1=2
f11=freq11/max tf1=2/2=1，因此，tf/idf加权模式w11=f11*idf1=1*log2=log2
文档索引词集		             文档向量
信息检索模型 (5)
提纲
信息检索概述
信息检索模型
文本信息检索
Web信息检索
信息检索评价指标
总结
文本信息检索 (1)
文本信息检索 主要对象是文本数据





词汇的分布规律
文本预处理
齐普夫分布模型
词频f=C/r,揭示词频和词排名之间的关系,存在“长尾”特征
词干提取
“ running, runs, ran ” → “ run ”
名词及名词性短语识别
“人工智能正在改变世界”→ “人工智能” ， “世界”
Heaps分布模型
词汇量V=Kt
揭示词汇量随文本长度变化的规律
文本词汇分析
“Python 3.8” →“ Python ”，“MP4” →“MP4” 
停用词去除
“人工智能正在改变世界” → “人工智能” “改变” “世界”
倒排索引的使用

文本信息检索 (2)

词汇表检索：对提问式q分词，在词汇表中进行检索
倒排表检索：提问式q中所有词对应的倒排表
倒排表操作：对检索出的倒排表进行后处理，实现检索查询

【例】查询词“agent AND James”所在的文档
倒排索引
文本信息检索 (3)
创建倒排索引的算法（基于内存的方法）
第一次遍历：
对每个词获得出现过该词的文档数
所需内存大小
第二次遍历：
获得词的文档号及在文档中出现的位置
快速更新词的倒排表
基于内存的方法是倒排索引构建的基础，基于此构建倒排文件
提纲
信息检索概述
信息检索模型
文本信息检索
Web信息检索
信息检索评价指标
总结



Web信息检索 (1)
  Web信息检索（针对互联网上数据的检索）


基本概念
网页去重
从海量、分散无序、动态变化等复杂数据中检索
PageRank排序
个性化PageRank
HITS排序
结果排序
Shingle去重
Simhash去重
Web信息检索 (2)
Single网页去重：根据网页的特征计算向量距离判断是否重复




 单词级别特征提取——
“政府计划增加教育预算”→ “政府”、“计划”、“增加”、“教育”、“预算”的词向量或词频
  Shingle级别特征提取
针对整个网页或网页的一个具体片段进行特征提取
网页的整体结构、新闻类别分布、页面布局信息等

 文档级别特征提取——
提取整个文章主题、情感分析结果、关键词或主要内容

A和B的相似度：
Web信息检索 (3)
Simhash网页去重：局部敏感哈希算法，对文档降维计算相似度

分词：对文本分词得到特征向量，利用tf/idf计算权重,输出<特征，权重>
加权、降维

哈希值计算：
输入<特征，权重>
输出<哈希值，权重>
距离计算


A和B的Simhash值的海明距离为2
（有两个位不同，具有一定相似性）
Web信息检索 (4)
Web页面排序的基本思想
链接反映网页之间形成的“参考”、“引用”和“推荐”关系
若一个网页被较多的其他网页链接，则它相对较被人关注，其内容应该是较重要、或者较有用的
一个网页的“入度”（指向它的网页的个数）是衡量它重要程度的一种有意义的指标。和科技论文情况类似，被引用较多的就是较好的论文
网页的“出度”（从它连出的超链个数）对分析网上信息的状况也很有意义的（门户网站），因此可考虑同时用两个指标来衡量网页的重要性
链接流行度（Link popularity）技术
    - 通过其他页面连接到当前页面的链接数量来决定当前页面的重要性
    - 防止人为加工的页面欺骗搜索引擎，由网页间的超链关系发现重要页面
Web信息检索 (5)
PageRank结果排序算法
如果一个页面被多次引用，则这个页面很可能是重要的
如果一个页面尽管未被多次引用，但都被一个重要的页面引用，则这个页面很可能是重要的
一个页面的重要性被均分，并传递到它所引用的页面








在有向图上利用随机游走算法，输出符合用户需求的网页排序结果
初始向量
初始转移矩阵
计算转移概率
规范化处理
Web信息检索 (6)
个性化PageRank排序 应对推荐系统类应用场景，根据用户喜好来推荐不同的商品，改进PageRank
等概率随机选择
网页节点

PageRank算法：等概率访问网页节点
升级
多轮游走后，依据概率对剩余节点进行排序，生成推荐列表


Web信息检索 (7)
HITS包含权威性和目录型网页，目录型网页提供了只想权威性网页的链接集合，权威性网页对于检索而言是高质量的内容网页


     q的检索结果
前n个页面记为基页S
迭代S引用+引用S页面
构建T

I操作：用网页v的Hub值修正u的Authority值O操作：用网页u的Authority值修正v的Hub值
提问式 q

基页
T







提纲
引例
信息检索概述
信息检索模型
文本信息检索
Web信息检索
信息检索评价指标
总结
信息检索评价指标 (1)
单个查询的评价指标

收录范围（Coverage）
查全率/召回率（Recall，简记为R）
查准率（Precision，简记为P）
响应时间（Response Time）
用户负荷（User Effort）
输出方式（Output Format）


文档集C和提问式q
q对应文档集R
q检索返回文档集A


检测出的相关文档集
相关文档集
检出文档集
信息检索评价指标 (2)



查全率和查准率的替代性指标

查准率-查全率曲线
查全率-查准率曲线更能清晰地描述评价结果
信息检索评价指标 (3)
多查询检索指标



信息检索评价指标 (4)
面向用户的检索指标




提纲
引例
信息检索概述
信息检索模型
文本信息检索
Web信息检索
信息检索评价指标
总结
总结
信息检索目的、主要流程和模型
信息检索模型的分类，布尔模型、向量空间的理论基础
文本信息检索中词汇的分布规律和倒排序方法
Web信息检索的工作原理，网页去重和排序的方法
信息检索评价指标（查准率、查全率、覆盖率、新颖率）
结语


谢谢！

=== 第3章_数据组织.pptx ===
第3章 数据组织 


《智能数据工程》

清华大学出版社
2025年1月
提纲
数据组织概述
数据仓库
数据湖
向量数据库
总结
数据组织内涵
定义：按照一定的方式和规则对数据进行归并、存储、处理的过程



引例：有如上表所示学生ID及成绩的数据，则数据组织的任务为：
① 进行数据异常处理、数据清洗等数据预处理操作
② 按照一定方式其存入相应的数据容器（可理解为数据库）
③ 对存入数据容器中的数据进行再处理
数据组织概述 (1)

数据组织方式发展脉络

数据组织概述 (2)
20世纪70年代：传统数据库系统
20世纪90年代：数据仓库
2010年前后：数据湖
2016年前后：数据中台
2020年前后：向量数据库




提纲
数据组织概述
数据仓库
数据湖
向量数据库
总结
数据仓库内涵
面向主题的、集成的、时变的、不可更新的数据集合
支持管理层的决策制定过程和商业智能
数据仓库基本特征
面向主题
集成
时变
不可更新
数据仓库 (1)
操作型与信息型系统对比
数据仓库 (2)
数据仓库 (3)




源数据系统
(内部数据、   外部消息)
数据准备区
(处理、清理、调和、导出、匹配、合并、转换)
数据和元数  据存储区

数据仓库
用户终端表示工具
(即席查询工具、报表生成器、用户应用程序、建模/挖掘工具、可视化工具)
抽取
装载
供给
(Extract)
(Load)
(Feed)
反馈
模型/查询结果
数据仓库体系结构
    ① 一般的两层结构，② 独立数据集市，③ 依赖数据集市和操作型数据存储，  
    ④ 逻辑数据集市和实时数据仓库
一般的两层结构：源数据系统+数据和元数据存储区
数据仓库的ETL工具——从一般的两层结构提取得到：
数据抽取
数据装载
数据清理
数据转换
数据仓库 (4)

源数据系统
(内部数据、   外部消息)

数据准备区
(处理、清理、调和、导出、匹配、合并、转换)
导出到数据集市

数据和元数  据存储区


数据集市
…

用户终端表示工具
(即席查询工具、报表生成器、用户应用程序、建模/挖掘工具、可视化工具)
反馈
模型/查询结果
抽取
抽取
抽取
装载
装载
装载

供给
独立数据集市的数据仓库体系结构
两层结构包含所有企业需要的主题信息
实际分析仅需部分主题，可在逻辑或物理上将其拆分为数据集市

数据集市
数据仓库 (5)

源数据系统
(内部数据、   外部消息)

数据准备区
(处理、清理、调和、导出、匹配、合并、转换)
导出数据仓库和数据集市

数据和元数据
存储区

数据集市

用户终端表示工具
(即席查询工具、报表生成器、用户应用程序、建模/挖掘工具、可视化工具)
反馈
模型/查询结果
抽取
抽取
抽取
装载

供给



数据集市
装载

依赖数据集市和操作型数据存储体系结构——三层结构
① 降低冗余数据及其处理的代价  ② 提供企业级数据视图
③ 具备向下钻取细节信息的能力  ④ 减小扩大规模和保持分离数据一致的成本
企业数据仓库

数据集市
数据仓库 (6)

源数据系统
(内部数据、   外部消息)

数据准备区（操作型数据存储）&数据和元数据存储区
(处理、清理、调和、导出、匹配、合并、转换)
一致维度加载到DW（数据仓库）

用户终端表示工具
(即席查询工具、报表生成器、用户应用程序、建模/挖掘工具、可视化工具)
反馈
模型/查询结果
抽取
抽取
抽取

供给

实时数据仓库
转换层
逻辑数据集市和实时数据仓库体系结构
针对中等规模的数据仓库
针对使用高性能数据仓库技术
数据仓库与数据集市对比
数据仓库 (7)
提纲
数据组织概述
数据仓库
数据湖
向量数据库
总结
数据湖概述
一种灵活的、可扩展的数据存储库和管理系统
以原始数据格式接收和存储数据
提供按需处理和数据分析的能力

数据湖 (1)
数据湖与数据仓库区别？
数据仓库与数据湖对比
数据湖 (2)
数据中台概述
对企业各个业务应用场景中形成的数据资源进行整合加工
以共享的方式将数据复用到不同业务
偏中上层数据服务设施，利用数据支撑企业的业务、服务、决策、运营
数据湖 (3)
数据湖 (4)

结构化数据
半结构化数据
非结构化数据





初始数据池
模拟数据池
应用程序数据池
文本数据池
归档数据池
数据湖体系结构——数据池体系结构
将摄取的所有原始数据存储在初始数据池中
将数据先放入初始数据，再放入到其他数据池中

模拟数据池
存储和处理半结构化数据，关键步骤为数据缩减
数据缩减
 - 数据去重：如何判断重复记录      
 - 数据采样
 - 数据压缩
数据湖 (5)





初始数据池
模拟数据池
应用程序数据池
文本数据池
归档数据池
相似重复记录的识别算法（匹配算法）
 可用于判断记录是否重复，主要方法：
字段匹配法（Field Matching）
编辑距离法（Edit Distance）
N-Gram算法（N-Gram）
将一个串转换为另一个串的过程中需插入、删除及替换字符个数的最小值
应用程序数据池
本质上是一个数据仓库
对不同应用程序产生的数据进行整合和转换

数据湖 (6)





初始数据池
模拟数据池
应用程序数据池
文本数据池
归档数据池
数据转换功能：
①记录级别功能
    选择、连接、规范化、聚合
②字段级别功能
    将数据从源记录中的给定格式    转换为目标记录中不同格式
文本数据池
存储和处理非结构化的文本数据
归档数据池
存储来自模拟数据池、应用程序数据和文本数据池中使用频率较小的数据
数据区域体系结构
数据湖 (7)
初始区域
黄金区域
敏感期区域
工作区域

结构化数据
半结构化数据
非结构化数据
体系结构的第一个存储区域
存储初始区域、黄金区域和工作区域的重要数据文件
存储对初始区域中原始数据进行数据清理和数据转换处理后的数据
从黄金区域中复制的数据根据业务需求进一步加工处理
Hudi（Hadoop Upserts Deletes and Incrementals）
开源的数据湖框架

数据湖 (8)
时间轴   
文件管理和索引   

表类型  
查询类型
数据湖 (9)
数据文件1
A,B
数据文件2
C,D
数据文件3
E
快照查询
时间戳=1
时间戳=2
时间戳=3
时间戳=4
日志文件1
A1
日志文件2
D1
日志文件1
A1，A2
日志文件3
E1，F
数据文件4
A2，B
数据文件5
C，D1
数据文件6
E1，F
增量查询
读优化查询
A,B,C,D,E
A,B,C,D,E
A,B,C,D,E
A1,B,C,D1,E
A1,D1
A,B,C,D,E
A2,B,C,D1,E1,F
A2,E1,F
A,B,C,D,E
A2,B,C,D1,E1,F
A2,B,C,D1,E1,F
Hudi三种查询方式在不同时间戳下的查询结果
提纲
数据组织概述
数据仓库
数据湖
向量数据库
总结
向量数据库概述
专门为存储、管理和查询高维向量数据而设计的数据库系统
有效处理大量高维数据点
提供高效的数据检索和相似性搜索能力
向量数据库 (1)
向量数据库特点
维度
特征表示
数据存储
查询机制
索引技术
向量数据库的索引技术
KD树（K-Dimension Tree）：一种特殊的二叉树，主要用于多维空间关键数据的搜索（如范围搜索和最近邻搜索）
局部敏感哈希（Locality-Sensitive Hashing, LSH）：一种用于高维空间数据近似最近邻搜索的哈希技术
分层导航小世界图（Hierarchical Navigable Small World, HNSW）：一种基于图的索引技术，结合了小世界网络和层次结构的优点，能在保持搜索效率的同时降低内存消耗
向量数据库 (2)
向量数据库的索引技术——KD树
KD树的构建
采用递归方式，从根节点开始，选择某一维进行划分，数据点按该维的值排序
选择中位数作为划分点，将数据点划分为两个子集，分别作为左子树和右子树的根节点
递归地在每个子集中选择新的维度进行划分，直到所有数据点都被划分到叶子节点为止
向量数据库 (3)
向量数据库的索引技术——局部敏感哈希
向量数据库 (4)
基本原理：若两个数据点在原始空间中相近，则其经过哈希函数映射后得到的哈希值也相近
哈希函数的选择——LSH的关键
可扩展性和处理高维数据的能力
哈希函数的选择和阈值的设定对性能影响较大，需根据数据的特性进行调优

向量数据库的索引技术——分层导航小世界图
结合了小世界网络和层次结构的优点，能在保持搜索效率的同时降低内存消耗
构建包括初始化和优化两个阶段
采用层次结构，将边按特征半径进行分层
搜索复杂度降到O(log n)，n为图中的节点数
向量数据库 (5)
向量数据库的搜索技术

向量数据库 (6)
近似最邻近搜索是最基本、最重要的操作之一 
相似性度量是判断两个向量是否相似的依据

欧几里得距离（Euclidean Distance）（最常用相似性度量方法）
余弦相似度（Cosine Similarity）
Jaccard系数（Jaccard Coefficient）
有没有牺牲效率来提高精度的方法？
向量数据库产品介绍——Milvus
专为大规模相似性搜索和向量索引设计的开源向量数据库
兼容多种索引类型，具备高效的搜索功能
非常适合应用于包括图像和视频识别、自然语言处理和推荐系统在内的多种人工智能(AI)和机器学习(ML)场景
支持基于布尔表达式的标量过滤和迭代器查询，方便处理大量数据
向量数据库 (7)
向量数据库产品介绍——Pinecone
一款云原生向量数据库
极大地简化了开发和扩展向量搜索应用的复杂性
采用分布式架构，支持水平扩展，确保服务高可用性和高性能
提供丰富的API接口
广泛应用于电商、新闻、社交、娱乐、教育、金融和医疗等领域
向量数据库 (8)
向量数据库产品介绍——Weaviate
开源的向量搜索引擎，支持向量化处理、数据分类和语义搜索功能
让向量搜索变得更加容易获取和扩展
支持查询时推理和导航，寻找最相关的实体和属性
结合自然语言处理技术和向量空间模型，支持多模态数据的语义搜索
具有强大的图谱构建、实体关联和语义搜索功能，在知识图谱和语义理解等方面有良好的应用前景

向量数据库 (9)
向量数据库产品介绍——Qdrant
 专为提升性能和灵活性而设计的开源向量搜索引擎
能够处理精确搜索和近似搜索，确保在不同人工智能(AI)和机器学习(ML)应用中实现准确性与速度的最优平衡。
具有卓越的查询速度和数据吞吐量，是大规模向量数据处理的首选工具
基于向量检索算法提供高性能的存储和查询能力
通过智能缓存机制来减少重复计算以提高查询性能，且支持并行处理
向量数据库 (10)
向量数据库产品介绍——VectorDB
向量数据库平台为深度学习从业者提供了卓越的性能和精准的向量数据存储与检索方案
基于向量相似度计算实现了对大规模多媒体数据的实时相似度检索
支持水平扩展，通过简单添加节点来增强存储和计算能力，确保在大数据量时仍能保持稳定的性能
具备广泛的适用性，使VectorDB能满足不同场景下的存储和检索需求
向量数据库 (11)
向量数据库产品介绍——Faiss
一个针对密集向量集合进行高效相似度搜索和聚类的库
通过优化算法和硬件加速实现高性能的相似性搜索和聚类，能处理大规模的向量数据集，并在毫秒级时间内返回相似度最高的结果
提供了多种索引算法，适用于不同的数据集和查询需求，支持多种相似度度量方法，使其能广泛应用于各种需要计算向量相似度的场景
支持分布式计算，可通过多机部署来扩展处理能力
向量数据库 (12)
提纲
数据组织概述
数据仓库
数据湖
向量数据库
总结
总结
数据仓库
数据仓库的概念和体系结构
数据集市和数据仓库对比
数据湖
数据湖的概念和体系结构
数据去重中的距离编辑算法
数据湖软件Hudi
向量数据库
向量数据库的索引技术、搜索技术
常用的距离度量方法
向量数据库产品
结语


谢谢！

=== 第4章_高维数据挖掘.pptx ===
第4章 高维数据挖掘
《智能数据工程》

清华大学出版社
2025年1月
提纲
引例
高维数据挖掘概述
数据降维
数据分类
数据聚类
总结

引例 (1)
MNIST数据集：60000个训练样本和10000个测试样本
  


2828像素手写数字图片的MNIST数据集
手写数字“1”的图片及相应的像素矩阵
 完成数据分类
 数据维度高，计算复杂度高
 可视化程度不高
手写数字“1”的图片识别模型训练过程
引例 (2)
对MNIST数据集降维


手写数字“1”的图片识别模型训练过程（假设降维后维度为2）
 完成数据分类
 压缩数据
 降低计算复杂度
 提高可视化程度

降低数据维度，保证其有效信息不丢失
引例 (3)
电商平台面临的实际问题
如何快速精准地实现用户分群？（预测流失或VIP客户）
如何预测新产品的销量及喜爱该产品的客户？
如何对客户的某些特征进行分类（圈选具有共同特征的用户，提供个性化的购物体验）
引例 (4)
共享单车停放点问题
共享单车分布示意图
共享单车停放站点示意图
空间上呈现数量多、较为聚集的特点

聚集区域可视为共享单车停放站点
利用聚类分析技术找到聚集区域中心点

提纲
引例
高维数据挖掘概述
数据降维
数据分类
数据聚类
总结

高维数据挖掘概述 (1)
高维数据挖掘关键问题
高维数据存在维度灾难问题，因此数据降维是高维数据挖掘的关键步骤
基于特征变换的降维技术是将高维空间中的数据通过线性或非线性映射投影到低维空间中，找出隐蔽在高维观测数据中有意义且能揭示数据本质的低维向量









高维数据的维度灾难问题
数据降维
基于特征变换的降维技术
线性降维
非线性降维
关键
常用技术
回忆一下第三章学习过的维度灾难是什么？






高维数据挖掘概述 (2)
基于特征变换的降维技术
 - 线性降维通常不能在降维过程中较好地保持数据集的非线性特性
 - 非线性降维技术通常基于线性降维技术进行非线性扩展或采用神经网络等方法







基于特征变换的降维技术
线性降维
非线性降维
主成分分析
奇异值分解
局部线性嵌入
自编码器
线性判别分析
等距特征映射
还知道哪些线性降维和非线性降维技术？




高维数据挖掘概述 (3)
数据分类
目的：根据新数据样本的属性为其分配一个正确的类别
应用：图片识别、信誉证实、医疗诊断、异常检测、情感分析…
经典的单一分类算法
决策树（Decision Tree）
k-近邻（k-Nearest Neighbor）
支持向量机（Support Vector Machine, SVM）
贝叶斯（Bayesian）分类
人工神经网络（Neural Network）
关联分类（Association Classification）
监督学习（Supervised Learning）
数据聚类
将一组给定的数据对象划分为多个互不相交的子集，每个子集称为一个簇（Cluster）
簇内数据对象之间相似度高，簇间数据对象之间差异性大

高维数据挖掘概述 (4)





























 高维数据挖掘概述（5）
定义数据对象之间的相似度
闵可夫斯基距离（Minkowski Distance）
欧氏距离（Euclidean Distance）
曼哈顿距离（Manhattan Distance）
聚类目标函数（聚类停止判别条件）
判断多个划分结果哪个是有效的
划分结果达到聚类目标函数时终止算法运行
簇别划分策略（算法）
通过何种簇别划分方式使得划分结果达到目标函数
提纲
引例
高维数据挖掘概述
数据降维
数据分类
数据聚类
总结

什么是降维相似度？
降低数据维度，保证其有效信息不丢失

为什么要降维？
缓解高维数据维数灾难问题
提高数据可视化程度
数据压缩减少存储空间
 数据降维 (1)
基于深度学习的降维方法
自编码器
变分自编码器（生成模型）
对抗神经网络 （生成模型）
传统的降维方法
主成分分析
奇异值分解
线性判别分析
不能较好地保持数据集的非线性特性
数据降维 (2)
将原始输入映射为低维数据，实现对输入数据的降维

自编码器基本思想
自编码器基本思想——编码阶段
数据降维 (3)
将低维数据映射成高维数据，实现对输入数据的重构
自编码器基本思想——解码阶段
数据降维 (4)
自编码器基本思想——重构误差
数据降维 (5)
自编码器基本思想——训练算法
数据降维 (6)
自编码器编码解码结构简单 
自编码器不能学习到服从隐变量的数据分布
自编码器中不能生成和原始输入相似的数据
自编码器概述
数据降维 (7)
变分自编码器是一个生成模型 
变分自编码器能学习隐变量所服从的概率分布，并通过概率分布采样生成和原始数据相似的数据，支持新样本的生成
变分自编码器概述
数据降维 (8)
变分自编码器——模型训练


从高斯分布中生成隐变量的随机采样样本z

数据降维 (9)
变分自编码器——损失函数优化目标
假设z服从多元高斯分布

Q(z|x)：编码过程学习到的概率分布

P(x|z) ：解码过程学习到的概率分布
数据降维 (10)
变分自编码器——损失函数优化目标
数据降维 (11)
变分自编码器——训练算法
提纲
引例
高维数据挖掘概述
数据降维
数据分类
数据聚类
总结

数据分类 (1)
贝叶斯分类的基本概念
以贝叶斯定理为基础、用概率论和统计学知识进行分类的算法
朴素贝叶斯分类、链增强朴素贝叶斯分类、树增强朴素贝叶斯分类等
朴素贝叶斯分类
贝叶斯分类器中最简单、应用最为广泛的算法之一
由于假设特征之间相互独立，所以称为“朴素贝叶斯” 
分类时对每个类别计算P(ck)P(xi|ck) ，以P(ck)P(xi|ck)的最大项作为待预测样本X所属的类别
数据分类 (2)
没有变量独立假设时计算需指数时间
数据分类 (3)
朴素贝叶斯分类的基本思想
设在给定类别变量下属性变量之间条件独立，朴素贝叶斯分类使P(ck|x1, …, xn)最大
在条件独立性假设下，朴素贝叶斯分类具有简单的星形结构网络结构
每个属性只有唯一的类ck作为其父节点，这意味着给定类ck时，x1, x2, …, xn条件独立，即
数据分类 (4)
朴素贝叶斯分类的基本思想
为了降低P(ck | x1, …, xn)的计算复杂度，根据条件独立性将联合概率分解为：


根据联合概率的分解形式，对于给定的待预测样本X，朴素贝叶斯分类形式表示为：

数据分类 (5)
数据分类 (6)
n(ck)为第ck类中样本的数量n(D)为样本总数
n(xi|ck)为第ck类中属性为xi的样本数量
n(ck)为第ck类中样本的数量n(D)为样本总数
数据分类 (7)
朴素贝叶斯分类的训练算法





时间复杂度O(n(ck)×n)
数据分类 (8)
朴素贝叶斯分类示例

任务：已知某人身高“高”、体重“中”和鞋码“中”，预测其性别。
设“男”和“女”为2个类别，用c1和c2表示；属性集合为“身高”、“体重”和“鞋码”，用x1、x2和x3表示。
数据分类 (9)
支持向量机的基本概念
二分类模型：在样本空间中找出一个超平面来对数据进行分类，并使分类误差尽可能小
分离超平面：比所在数据空间小一维的空间，在二维数据空间中是一条直线，在三维数据空间中就是一个平面

数据分类 (10)
数据分类 (11)
数据分类 (12)

数据分类 (13)
实际情况下几乎不存在
数据分类 (14)
数据分类 (15)
支持向量机训练算法

    

时间复杂度 O(n3)
空间复杂度O(n2)
数据分类 (16)
核函数
原始样本空间可能不存在能正确划分两类样本的超平面
经过空间转换，在高维空间解决线性问题等价于在低维空间中解决非线性问题
提纲
引例
高维数据挖掘概述
数据降维
数据分类
数据聚类
总结

数据聚类 (1)
传统聚类算法
基于划分的聚类算法
先将数据集任意划分为k个不相交的簇
迭代优化逐步改善簇的划分
目标函数收敛时，得到最终的聚类结果
       k-均值（k-Means）算法、最大最小距离（Max-Min Distance）算法
基于密度的聚类算法
通过数据密度（单位区域内的实例数）来发现任意形状的类簇


数据聚类 (2)
传统聚类算法——层次聚类算法
自底向上的聚合型层次聚类
先将每个数据对象作为一个聚类簇
计算簇间的相似度进行分层合并，直至最后只有一个簇或满足目标函数时终止
自顶向下的分裂型层次聚类
先将所有数据对象看作一个聚类簇
逐层分裂，直至每个簇中只包含一个数据对象或满足满足目标函数时终止


数据聚类 (3)
传统聚类算法
基于网格的聚类算法
先将数据空间划分为网格单元，并将数据对象映射到网格单元
判断每个网格单元是否形成类簇
基于模型的聚类算法
为每个聚类假设一个模型
发现符合模型的数据对象


数据聚类 (4)
智能聚类算法
大数据聚类算法
分布式聚类（Distributed Clustering）算法
使用MapReduce框架对传统聚类算法进行扩展
并行聚类（Parallel Clustering）算法
使用并行框架对传统聚类算法进行扩展
基于深度学习的聚类算法
利用深度学习模型将高维的原始数据映射为低维特征向量
再利用特征向量进行聚类

数据聚类 (5)
𝐽(𝐶)值在一定程度的上刻画了簇内数据对象围绕簇中心点rj的紧密程度，𝐽(𝐶)值越小，簇内数据对象相似度越高
rj 是簇cj的均值向量
数据聚类 (6)
时间复杂度： O(nkt)
t为迭代次数
k-均值算法步骤
指定需要划分簇的个数k值
随机选择k个数据对象作为初始簇中心点
计算其余数据对象到k个簇中心点的欧式距离，将其划分到最近的簇中
调整新簇，并重新计算每个簇的平均值
计算聚类目标函数𝐽(𝐶)，若不满足收敛条件，重复步骤2~4

数据聚类 (7)
 



数据聚类 (8)





K-均值聚类示例
数据聚类 (9)








基本步骤
  - 网格划分
  - 稠密单元识别
  - 候选网格剪枝
  - 簇的发现
核心：稠密单元的识别和候选网格的剪枝
数据聚类 (10)









CLIQUE聚类算法流程
数据聚类 (11)

数据聚类 (12)

 
数据聚类 (13)


谱聚类基本思想
基于样本间的相似性构造图，对图的拉普拉斯矩阵进行特征分解，在分解得到的特征上使用k-均值算法完成聚类
目标：子图内各节点间有较高的边权重和，各子图之间具有较低的边权重和
理论推导后得到问题是NP难问题         只能使用特征向量去近似求解
优势：能处理高维数据及非凸形状的簇，在复杂数据上比k-均值更好的聚类效果
劣势：依赖于所构建的相似性图，特征值分解不易扩展到大规模数据上，需用户指定聚类的簇数目k


数据聚类 (14)


对k-均值聚类进行扩展，能处理高维及非凸数据
数据聚类 (15)

使用特征向量来近似计算，通过牺牲精度换取计算效率
谱聚类算法
数据聚类 (16)

提纲
引例
高维数据挖掘概述
数据降维
数据分类
数据聚类
总结

自编码器
优点：重构过程简单、可学习数据的有效表示且无需标注数据
缺点：具有不可解释性，可能过拟合
变分自编码器
优点：能显式地构建样本的概率分布、具有生成新样本的能力
缺点：训练复杂度高、性能对超参数敏感、生成质量依赖模型设计
朴素贝叶斯
优点：时空开销小，可处理多分类任务，对缺失数据不太敏感，结果可解释
缺点：决策存在错误率，对输入数据的表达形式很敏感
支持向量机
优点：全局最优值，泛化能力强，算法简单且鲁棒
缺点：样本数量大时，存储和计算的开销较大
总结 (1)
K-均值聚类
优点：简单易懂、算法效率高、在很多实际应用中表现都良好，具备较好的收敛性
缺点：对初始中心敏感，需指定簇数、对噪声和离群点敏感
CLIQUE聚类
优点：能处理高维数据，能自动确定簇的数量，也易于解释
缺点：对参数敏感，计算复杂度高，高维度诅咒
谱聚类
优点：能处理非凸形状簇，适合处理高维数据，效果也比较稳定
缺点：对参数敏感，计算复杂度高，需预先指定簇数，存储需求高
总结 (2)
结语


谢谢！

=== 第5章_视觉数据分析.pptx ===
第5章 视觉数据分析


《智能数据工程》

清华大学出版社
2025年1月
提纲
概述
目标检测
图像分割
视频目标跟踪
总结
概述
视觉数据分析以图像或视频作为输入，通过计算机视觉技术构建模型，旨在充分提取图像和视频数据的特征，进而根据特定任务利用提取的特征完成相应的功能。




视觉数据分析任务的关键：对图像和视频数据特征提取
图像分类
目标检测
图像分割
视频目标跟踪
卷积神经网络（CNN）用于视觉数据特征提取的主流模型
提纲
概述 
目标检测
图像分割
视频目标检测
总结

目标检测（Object Detection） 
利用矩形边界框来确定图像中目标所在的位置及大小，识别目标所属的种类，并给出相应的置信度
应用场景
交通领域：交通违法事件检测
医学领域：病变细胞检测
工程领域：安全帽佩戴检测
工业领域：绝缘子检测
农业领域：农作物病虫害检测
目标检测 (1)
目标检测 (2)
卷积神经网络（Convolutional Neural Network, CNN）
   卷积神经网络的层级结构主要包含输入层、卷积层、池化层、全连接层等，通过层级的组合可构建不同的卷积神经网络
优点
与全连接神经网络相比，层与层之间稀疏的局部连接减少了参数数量
共享的卷积核参数有助于捕捉图像的局部特征


卷积层
池化层
全连
接层

···

多个卷积层、池化层交叉堆叠
预测结果
卷积层
池化层
输入层
目标检测 (3)
卷积层
   卷积核上的参数和窗口对应区域内的像素值进行乘法求和运算并加上     偏置项，得到输出特征图对应位置的特征值

卷积
+
偏置项
卷积核
卷积结果
0×5+3×3+2×3+1×2+2×1+1×4+3×2+3×1+0×9 +3 = 35
目标检测 (4)
池化层
对输入特征图进行下采样 ，将子区域内的特征值压缩成一个能表示该区域的特征值，整合邻域特征，减少参数和计算量
常见的池化操作包括最大池化和平均池化

最大池化
平均池化
原特征图
池化结果
目标检测 (5)
全连接层
  将高维特征图映射为低维数据









输入图像
CNN
   7×7×256
   1×1×12544
F
C

   1024
1024个7×7卷积核
   1×1×1024
···
传统全连接操作
卷积替代全连接
由Joseph Redmon和Ali Farhadi等于2015年提出
与R-CNN等二阶段算法不同
仅基于单个CNN，并将特征提取和检测框定位两个步骤相结合
可直接从完整的图像中预测检测框、得到分类置信度

YOLO算法概述
YOLO算法训练 (1)
图像缩放
基于YOLO的网络结构，需将输入图像的长宽像素缩放为固定值448×448
先将图片中最长的边缩放到448像素
再对短边的空白位置补上灰色 
1620
376
448
256


448
448
缩放

填充像素
YOLO算法训练(2)
设置网格及网格中检测框个数
把经缩放的图像划分为S×S个网格（Grid）
每个网格中设置B个检测框
例如，将S和B分别设置为7和2，即将图像划分为7×7个网格，每个网格有2个检测框
S为7，将图像划分为7×7的网格
448
B为2，有2个检测框负责检测目标物体



YOLO算法训练 (3)
网络模型构建

s表示步长,p表示填充值


YOLO算法训练 (4)
非极大值抑制过程
从检测框集合中取出最大置信度对应的检测框A并作为结果输出
逐一计算其余检测框与检测框A的IoU，将IoU大于给定阈值（阈值一般设为0.5）的检测框从检测框集合中移除
重复上述2个步骤直至检测框集合为空


cat
B ：0.6
C ：0.65
A ：0.75

A
取出置信度最大的检测框A作为结果输出
分别计算A与B、C的IoU
把IoU大于阈值的检测框
从检测集合中移除
重复上诉步骤直至检测框集合为空

YOLO算法示例 (1)
示例
以基于YOLO算法预测图像中的鸟类为例，将输入图像缩放448×448×3，设S=7，即划分为(7×7)个网格
448
448
S为7，将图像划分为7×7的网格
YOLO算法示例 (2)
将处理后的图像输入经过训练的YOLO网络模型中进行前向传播
每个网格采用2个检测框检测目标物体（B=2）
由于目标物体仅有鸟类，因此C=1，最终输出特征图的维度为7×7×(2×5+1)
输入448×448×3图像
输入YOLO网络模型进行预测
输出7×7×(2×5+1)的特征图


YOLO算法示例 (3)
输出特征图共预测出7×7×2 (98)个检测框及相应置信度
将98个检测框通过NMS方法进行筛选
最终得到网格4-5所对应的检测框为最终结果
输入448×448×3图像
输入YOLO网络模型进行预测
NMS方法
筛选出最匹配的检测框
提纲
概述 
目标检测
图像分割
视频目标跟踪
总结
图像分割（Image Segmentation） 
根据特征把图像划分成若干个互不相交的区域，使得这些特征在同一
区域内表现出一致性或相似性，而在不同区域间表现出明显的不同
应用场景
交通领域：无人驾驶
医学领域：医学图像分析
娱乐领域：电影特效
农业领域：智能遥感
图像分割 (1)
Mask R-CNN (1)
框架
第一阶段：由CNN提取特征，RPN筛选留下RoI，得到统一维度输出
第二阶段：统一维度的RoI添加FCN分支预测掩膜图
Mask R-CNN结构图

Mask R-CNN (2)
RPN结构
RPN为特征图上各个位置的每个锚框生成两个输出
一是锚框的前景概率和背景概率
二是锚框精度调参，以精调锚框来更好地拟合目标
RPN结构图

Mask R-CNN(3)
RoI对齐
将RoI与原特征图对齐并统一维度大小
例如：特征图尺寸为5×7的RoI统一缩小为2×2
划分两次确定最小区域中心点，插值计算中心点的值。把其中2×2区域内取4个中心点值中的最大值，作为该区域的值，得到统一维度输出。

RoI对齐流程图
Mask R-CNN (4)
FCN结构
FCN是一个端到端的网络
执行过程包括卷积和转置卷积
  
FCN结构图


Mask R-CNN (5)
Mask R-CNN推理的基本步骤
待分割图像预处理
提取图像的特征
生成锚框集合
候选RoI分类
对RoI进行RoI对齐
对RoI进行分类、边界框回归和分割
  


Mask R-CNN算法示例 (1)
示例
以Mask R-CNN算法分割桌面图像为例，将输入图像缩放1024×1024×3，为每个像素预设3个锚框
1024
1024
待分割图像X
Mask R-CNN算法示例 (2)
提取特征和生成锚框集合
使用基于CNN实现的ResNet101提取特征
以特征图宽高维度32×32为例，每个像素点设置3个候选锚框，所以共需设置1024×3个锚框，得锚框集合Bbox
  


Mask R-CNN算法示例 (3)


Mask R-CNN算法示例 (4)


提纲
概述
目标检测
图像分割
视频目标跟踪
总结

视频目标跟踪（Video Object Tracking） 
图像视频目标跟踪要求已知视频第一帧目标的初始位置，对该目标在后续视频帧中进行持续的跟踪定位
应用场景
行为分析：捕捉和分析人体动作，提供精准的行为分析数据
安全监控：监控摄像头系统对人群实时监控，识别和跟踪可疑行为

视频目标跟踪 (1)
典型的单目标跟踪算法
使用孪生网络结构进行相似度比较
优点：具有实时性，能够有效解决目标小范围晃动、运动模糊、短时局部遮挡的问题
Siamese FC算法概述
视频目标跟踪 (2)
Siamese FC框架
一张模板图像𝐙输入选取视频的第一帧，一张查询图像X选取视频后续帧
使用孪生网络提取特征后得到特征图𝜑(𝐙)和𝜑(𝐗)
以𝜑(𝐙)作为卷积核，对𝜑(𝐗)执行互相关计算得到相似度矩阵

Siamese FC基本步骤 (1)

Siamese FC基本步骤 (2)



Siamese FC基本步骤 (3)


Siamese FC算法示例 (1)
Z和X的构建
以跟踪连续视频帧中的猫为例，将第t帧缩放127×127×3作为模板图像Z，第t+1帧框选255×255×3作为查询图像X


第t帧视频帧
600×400×3
第t+1帧视频帧
  600×400×3
Z
127×127×3
X
255×255×3
Siamese FC算法示例 (2)
Siamese FC算法示例 (3)
提纲
概述
目标检测
图像分割
视频目标跟踪
总结

总结
视觉数据分析的基本思想、应用场景和常见方法
目标检测的重要算法实例：
 CNN的基本操作、模型结构
目标检测方法YOLO的基本思想、模型结构与训练步骤
 基于YOLO算法预测图像中的鸟类
图像分割的重要算法实例：
 Mask R-CNN 算法的基本思想、模型结构与训练步骤
基于Mask R-CNN 算法对桌面图像分割
视频目标跟踪的重要算法实例：
 Siamese算法的基本思想、模型结构与训练步骤
基于Siamese算法跟踪连续视频帧中的猫
结语


谢谢！

=== 第6章_文本数据分析.pptx ===
第6章 文本数据分析


《智能数据工程》

清华大学出版社
2025年1月
提纲
引例
文本数据分析概述
语言模型
情感分析
机器翻译
总结












引例  (1)
典型的问答系统场景
  

我当前使用的是多少的套餐？
您当前使用的是七十八套餐，包含20GB流量和300分钟通话时间。
q1
我每个月使用多少流量？
平均每个月使用25GB流量。
q2
目前最适合我的套餐是什么？
九十八套餐，它包含30GB流量和500分钟通话时间。
q3
引例  (2)
q1 和q2 可直接查询知识库获得答案
q3 需利用问答系统求解
（1）提取q1和q2的上下文语义信息
（2）识别实体“七十八套餐”
（3）链接知识库，确定与该实体对应的子集，生成候选答案集矩阵
（4）计算候选答案中与问题相似度最高的实体

提纲
引例
文本数据分析概述
语言模型
情感分析
机器翻译
总结
文本数据
以文本形式表示的数据，包含自然语言中的文字、字符和符号等信息，通常以字符串的形式存在
由一个或多个字符组成，可包含多个句子、段落或文档，广泛存在于万维网页、新闻报道、社交媒体、产品评论、科学文献等多个领域中
文本数据分析概述

报道、文献…

知识…


文本数据分析——通常指对文本进行特征提取和统计分析，尽可能地挖掘蕴含在文本中的知识，充分发挥文本数据的作用
常见的文本建模分析任务：
文本数据分析概述

穿着圣诞毛衣的
小刺猬牵着狗散步
文档       →        摘要
情感分析
文本生成
摘要
生成
文本分类
提纲
引例
文本数据分析概述
语言模型
情感分析
机器翻译
总结
语言模型 (1)
语言模型：对语言中的字符、词汇等进行特征表示，包含语法、结构、统计规律等丰富的语言知识和规律


340M
1.8T
94M
语言模型 (2)

垃圾短信分类
单词标注
机器翻译
语言模型 (3)
传统语言模型（神经网络语言模型）
Word2Vec 是用于生成词向量的语言模型，通过预测单词的上下文将单词表示为连续空间中的向量，捕获单词间的语义语法关系

CBOW
最大化单词的条件概率使得上下文单词的情况下预测当前单词的概率最大化
Skip-gram
最大化上下文单词的条件概率使当前单词的情况下预测上下文单词的概率最大化
语言模型 (4)
BERT预训练语言模型（以Transformer为基础）             



（1）Transformer概述（编码器+解码器）
编码器：将输入序列转化为上下文相关表示
解码器：根据编码器的输出和自身的输入逐步生成目标序列
多头注意力增强了模型的表达能力，通过并行多个注意力头捕捉不同的语义
引入位置信息
使用位置嵌入将每个输入位置与其相应的位置向量进行映射
位置嵌入向量通过正弦和余弦函数的组合来生成对应位置信息的表示



语言模型 (5)
BERT预训练语言模型（以Transformer为基础）    



BERT网络结构：
双向Transformer预训练模型+多层Transformer编码器
文本表示和建模
BERT输入案例：
文本序列输入到BERT前，使用词元嵌入、分段嵌入与位置嵌入构建初始向量
（2）BERT模型结构
语言模型 (6)
BERT预训练语言模型（以Transformer为基础）    



提纲
引例
文本数据分析概述
语言模型
情感分析
机器翻译
总结


情感分析 (1)
情感分析（ Sentiment analysis ）通过对文本进行建模分析，确定其中包含的正面、负面或中性等情感倾向

篇章级
对整个文本进行分析，关注整体的情感倾向
句子级
对单个句子进行分析，关注句子的情感倾向
属性级
对特定属性进行分析，关注文本中与某个特定属性相关的情感倾向

基于情感词典
根据情感词典所提供情感词的情感倾向，实现不同粒度的情感倾向划分
基于深度学习
利用深度神经网络自动学习文本特征，预测倾向
情感分析 (2)

情感分析 (3)
基于BERT的情感分析方法通过预训练+微调形式实现，利用大规模无标签数据和少量带标签数据，能够在特定的情感分析任务上取得较好的性能





情感分析 (4)

提纲
引例
文本数据分析概述
语言模型
情感分析
机器翻译
总结



机器翻译 (1)
机器翻译 利用计算机将源语言输入字符序列翻译成另一种语义等价的目标语言字符序列的过程

文档翻译
音频翻译
拍照翻译


我爱中国

你好
机器翻译 (2)
基于LSTM的神经机器翻译 通过编码器将句子映射为固定维度的向量，再使用解码器将编码后的向量进行解码，得到目标语言句子


机器翻译 (3)
【例】问答对q=“世界最高峰”、a=“珠穆朗玛峰”，LSTM提取文本序列特征：
      


	






机器翻译 (4)


基于BERT的机器翻译模型通过注意力机制充分学习所有隐藏状态以动态表示不同时刻的上下文向量，并通过渐进蒸馏等方式实现机器翻译


1.预处理

2.编码

3.解码
渐进蒸馏 将BERT作为教师网络，Transformer编码器做学生网络，对BERT进行训练
提纲
引例
文本数据分析概述
语言模型
情感分析
机器翻译
总结
总结
文本数据分析的目的、应用场景
语言模型（统计语言模型、神经网络语言模型、BERT语言模型）
情感分析方法（基于情感词典、基于BERT）
机器翻译模型（基于LSTM、基于BERT）
结语


   谢谢！

=== 第7章_图分析算法.pptx ===
第7章 图数据分析 


《智能数据工程》

清华大学出版社
2025年1月
提纲
引例
图数据分析概述
图神经网络
节点分类
链接预测
社区发现
评价指标
总结

引例 (1)
论文分类
以机器学习领域论文为例，论文可基于“遗传算法”、“神经网络”、“理论研究”等主题划分为多类
面对大量论文时，人工标注所有论文的类别变得非常困难
paper1
paper2
paper4
paper3
???




v1
v2
v3
v4


图节点分类问题
c2
c1
c2
???

v4
c2
引例 (2)
论文检索
以论文检索为例，不同领域论文间的引用关系、方便读者高效地搜索论文
面对论文的数量巨大，无法将全部论文的引用关系添加到网络中，使得读者搜索论文的效率变低
paper1
paper2
paper4
paper3




v1
v2
v3
v4


图链接预测问题
c2
c1
c2
c2
???




v1
v2
v3
v4
c2
c1
c2
c2
???
引例 (3)
论文社区检测
学术论文作为各领域学者的研究成果记录，能体现作者所在团体的研究领域和研究水平，但学者通常归属于代表不同研究领域的学术团体，学者想要了解其他领域学术成果或前沿技术，可能需付出较高的查询代价
paper1
paper2
paper4
paper3




v1
v2
v3
v4


图社区发现问题
c2
c1
c2
c2




v1
v2
v3
v4
c2
c1
c2
c2


???
???

提纲
引例
图数据分析概述
图神经网络
节点分类
链接预测
社区发现
评价指标
总结

图分析目的
  挖掘图数据中的知识，为基于图数据的分析应用提供支撑
图分析任务
节点分类（节点级）
   论文分类
链接预测（边级）
   预测部分论文间的引用关系、学术社交网络的补全
社区发现（图级）
   建论文引用关系图来发现论文社区


图数据分析概述 (1)

fri
tea
tea
sup
tea
cla
cla
学术社交关系预测

社区发现
××社区




v1
v2
v3
v4

c2
c1
c2
???

v4
c2
论文分类


图数据分析概述 (2)
优点：
算法易实现、直观
缺点：
无法保证收敛性
没有充分利用其他信息，如节点特征、边特征和图特征

图数据分析概述 (3)
图分析
基于图神经网络的算法
图卷积神经网络（GCN）
对边特征、节点特征、图特征进行
      聚合及更新操作
图注意力网络（GAT）

为每条边加上可学习的系数，进行带注意力的节点特征融合，使得模型在卷积过程中能够根据任务实时调整系数	
带注意力的节点特征融合

图数据分析概述 (4)
图分析：基于图神经网络的算法
优点：
充分利用图中的信息，有效地提取的节点特征
从节点、边和图层面实现高效的表示学习
学习到更加丰富的语义信息
缺点：
无法通过堆叠神经网络层数来获得更好的性能
节点特征计算的代价将非常高昂
    
	
提纲
引例
图数据分析概述
图神经网络
节点分类
链接预测
社区发现
评价指标
总结


图神经网络 (1)
图神经网络（Graph Neural Network, GNN）
概述
一类用于图数据建模与分析的神经网络
利用图卷积操作聚合信息，得到节点、边和图的特征
分类
消息传播神经网络（Message Passing Neural Network, MPNN）
非局部神经网络（Non-Local Neural Network, NLNN）
归纳
图网络（ Graph Network, GN）是GNN结构的一般化总结

图神经网络 (2)

图神经网络 (4)
图神经网络 (5)
使用GNN进行图分析处理的基本步骤
定义损失函数
      根据具体图分析任务类别定义损失函数
搭建模型结构
      根据任务输入和目标输出搭建模型结构，包括输入层、图卷积层和输出层
训练模型
      基于损失函数和梯度下降法设计模型训练算法，更新图卷积层的参数
实现图分析任务
      基于训练好的GNN模型实现具体的图分析任务

提纲
引例
图数据分析概述
图神经网络
节点分类
链接预测
社区发现
评价指标
总结


节点分类 (1)
节点分类
概述
     图节点分类是指，对于给定的图，根据图中部分已经标注的节点，对未标注 
     的节点进行标注，属于有监督的分类任务
分类
传统方法：
     关系分类（Relational Classification）、迭代分类（Iterative Classification）和
      信念传播（Belief Propagation）
深度学习方法：
      基于GCN的图节点分类和基于GAT的图节点分类方法

节点分类 (2)

节点分类 (3)

节点分类 (4)

对图中所有节点的一阶邻居特征进行聚合，
等价于将邻接矩阵与特征矩阵相乘

节点分类 (5)
图节点分类预测过程
提纲
引例
图数据分析概述
图神经网络
节点分类
链接预测
社区发现
评价指标
总结


链接预测 (1)
链接预测
概述
链接预测针对给定图中节点间的关系及节点属性来预测两个节点之间是否存在边，进而预测网络中实体间是否存在关系，属于有监督的二分类任务。
分类
基于相似性的方法
基于降维技术（Dimensionality Reduction）的方法
基于GNN的方法
（编码器-解码器架构，基于GCN的链接预测算法）

链接预测 (2)

链接预测 (3)
基于GCN的链接预测
- 用于链接预测的GCN模型主要包括输入层、编码器和解码器
- 通过编码器对节点特征进行降维，得到图卷积层特征
- 利用解码器计算节点间的点积和，得到重构邻接矩阵
- 通过重构邻接矩阵可得到节点间链接预测结果

链接预测 (4)

链接预测 (5)
图链接预测模型示例


图卷积层
Dropout层
解码器

编码器




阈值：0.5
提纲
引例
图数据分析概述
图神经网络
节点分类
链接预测
社区发现
评价指标
总结


社区发现 (1)
社区发现
社区发现（也称社区检测）任务，针对给定图中节点间的关系及节点属性来挖掘图中潜在的社区结构，属于无监督的聚类


传统方法：
     图分割、层次聚类、统计推断、动态技术、谱聚类及优化器等

深度学习方法：
      基于CNN和GNN的端对端模型
CNN处理不完整图，但需要对数据预处理
GCN既保留了CNN的优势，弥补了CNN不能直接处理图的缺陷

社区发现 (2)
基于GCN的社区发现
优化图卷积层权重，使得每一个节点连接同一社区中其他节点的边数都不小于该节点连接其他社区中节点的边数

社区发现 (3)
H0=A

社区发现 (4)
模型预测社区结构示意图









图或网络中不同的“簇”反映了图中不同节点之间连接的紧密程度
 示例网络
社区分配矩阵R
 权重矩阵W
根据R得到v1～5属于社区1，v8～10属于社区2
提纲
引例
图数据分析概述
图神经网络
节点分类
链接预测
社区发现
评价指标
总结


评价指标 (1)

评价指标 (2)

评价指标 (3)
AUC（Area Under Curve）
     不受阈值影响，可直观反映模型的整体预测能力

评价指标 (4)

评价指标 (5)
Z表示社区算法的划分结果，Y表示真实的划分结果
||Z||和||Y||分别表示划分后社区的数量，D表示的混淆矩阵
Dab表示Z划分中属于社区a而Y划分中属于社区b的节点数量
Za和Yb分别表示两次划分结构中社区a和b的节点数量
提纲
引例
图数据分析概述
图神经网络
节点分类
链接预测
社区发现
评价指标
总结

总结
图数据分析目的、任务和方法
图神经网络的分类，节点级、边级和图级的更新操作
使用图神经网络进行图数据分析的基本步骤
     - 基于GCN的图节点分类
     - 基于GCN的图链接预测
     - 基于GCN的图社区发现
评价指标（准确率、精确率、召回率、F1分数、ROC、AUC、模块度、归一化互信息）
结语


谢谢！

=== 第8章_知识图谱.pptx ===
第8章 知识图谱
《智能数据工程》

清华大学出版社
2025年1月
提纲
引例
知识图谱概述
知识图谱构建
知识图谱嵌入
知识图谱推理
总结

引例 
知识存储困难
信息技术高速发展和web 2.0的迅速普及，真实世界数据呈指数级增长
采集的数据规模已达到ZB级别，数据来源多样（文本、图片和视频）

如何全面有效地存储和表示知识？
知识表示困难
事物关联错综复杂
知识载体多源异构
提纲
引例
知识图谱概述
知识图谱构建
知识图谱嵌入
知识图谱推理
总结


知识图谱概述 (1)
 什么是知识图谱
知识图谱（Knowledge Graph, KG）是一种用图模型描述知识和建模世界万物之间关联关系的技术方法
 知识图谱的应用
智能问答，自然语言理解，大数据分析，推荐计算，可解释人工智能
 知识图谱任务
KG构建：利用自动或半自动的技术，从已有数据抽取知识要素（事实）
KG嵌入：将高维、稀疏的KG嵌入到低维稠密的向量空间
KG推理：从KG已有的知识出发，推断出新的或未知的知识

知识图谱概述 (2)

在古典名著《西游记》中，“菩提老祖和唐三藏是孙悟空的师傅”可由“<孙悟空，师傅，菩提老祖>”和“<孙悟空，师傅，唐三藏>”两个三元组表示
提纲
引例
知识图谱概述
知识图谱构建
知识图谱嵌入
知识图谱推理
总结


知识图谱构建 (1)
核心任务
获取实体及其对应关系组成的三元组
不同粒度划分
命名实体识别（Named Entity Recognition, NER）
关系抽取（Relation Extraction , RE）
实体关系联合抽取（Joint Extraction）
归纳
（半）结构化数据
非结构化数据
基于BERT-BiLSTM-CRF的NER方法
基于BERT-BiLSTM-Softmax的关系抽取方法
基于BiLSTM-LSTM-Softmax的联合抽取方法
非结构化数据规模远超结构化数据和半结构化数据

知识图谱构建 (2)
命名实体识别概念
从非结构化文本数据中抽取出具有特定意义的实体，主要包括人名、地名、组织名、机构名、日期、时间、货币和专有名词等
命名实体识别方法分类

基于规则和词典的方法：通常采用人工制定的规则进行实体识别
基于统计机器学习的方法：序列标注任务，“特征模板+CRF”
基于深度学习的方法：表示层，编码层，解码层
“孙大圣认得他，即叫：‘师傅，此乃是灵山脚下玉真观金顶大仙，他来接我们哩’。三藏方才醒悟，进前施礼”
人名：孙大圣、金顶大仙、三藏
机构：玉真观
地名：灵山


知识图谱构建 (3)

基于BERT-BiLSTM-CRF的命名实体识别
以BERT作为基础输入层编码方式，充分挖掘出字符的深层特征
以BiLSTM作为编码层模型自动提取序列特征并捕获输入序列中字符的上下文关联和长距离依赖关系
以CRF作为解码层模型保证输出标签序列满足现实约束，提升输出序列标签的正确性
将NER视为序列标注任务的端到端模型

知识图谱构建 (4)
标签标注
目前常用的序列标注方式包括BIO、BMES和BIOES三种：
BIO方式：“B-X”代表“X”类型实体的开头，“I-X”代表“X”类型实体的中间或结尾，“O”代表不属于任何类型实体
BMES标注方式：“B-X”代表“X”类型实体词首位置，“M-X”代表“X”类型实体的中间位置，“E-X”代表“X”类型实体的末尾位置，“S-X”代表单独字符的实体
BIOES标注法：“B-X”代表“X”类型实体词首位置，“I-X”代表“X”类型实体的中间位置，“E-X”代表“X”类型实体的末尾位置，“S-X”代表单独字符的实体，“O”代表不属于任何类型实体
知识图谱构建 (5)
基本步骤（1）
知识图谱构建 (6)
基本步骤（2）

知识图谱构建 (7)
关系抽取
关系抽取（RE）旨在命名实体识别的基础上进一步得到实体之间的关联
基于规则的RE方法
      通过人工构造规则以匹配文本来实现关系的提取
基于统计机器学习的方法
      通过对数据中不同特征进行分析和处理来实现实体间关系的分类
基于深度学习的方法
      远程监督，监督学习
RE方法分类

知识图谱构建 (8)
基于BERT-BiLSTM-Softmax的关系抽取

通过BERT语料库预先得到句子中字符的向量表示
通过BiLSTM学习得到其上下文关系
引入注意力层，通过生成权重向量并进行加权平均等操作完成句子整体表示
通过Softmax分类器判断该句子中实体对之间的关系

知识图谱构建 (9)
知识图谱构建 (10)

知识图谱构建 (11)
实体关系联合抽取
实体关系联合抽取旨在同时识别出文本中的实体及其存在的关系，是上述NER和RE两项子任务的结合
实体关系联合抽取方法分类
基于参数共享的方法
      NER子任务和RE子任务通过共享联合模型的编码层来进行联合学习
基于序列标注的方法
      
将NER和RE两个子任务变为一个统一的序列标注问题，同时识别出实体及其对应关系

知识图谱构建 (12)
基于BiLSTM-LSTM-Softmax的联合抽取

通过Word2Vec表示层获取句子中字符的基础向量表示
通过BiLSTM进一步学习上下文特征
通过LSTM层加强句子中字符的依赖关系并进行解码
通过Softmax分类器判断字符对应的标签类型



知识图谱构建 (13)
“B-R-$”代表实体词首位置，“I-R-$”代表实体词中间位置，“E-R-$”代表实体词结尾位置，“S-R-$”代表单独字符的实体，“O”代表非实体字符，“R”代表关系类型，“$”为头尾实体表示符，当“$”为1时表示该实体为头实体，“$”为2时表示该实体为尾实体
知识图谱构建 (14)
提纲
引例
知识图谱概述
知识图谱构建
知识图谱嵌入
知识图谱推理
总结


知识图谱嵌入 (1)
核心任务
将实体和关系映射到低维向量空间，实现对实体和关系的语义信息表示，从而高效地计算实体、关系及其之间的复杂语义关联
模型分类
距离（Distance-based）模型：


双线性（Bilinear）模型：
距离模型将关系建模为从头实体到尾实体的翻译，并通过变换后的距离差来定义评分函数
双线性模型通过矩阵分解完成实体和关系的嵌入学习

知识图谱嵌入 (2)
TransE（距离模型）

TransE模型是经典的距离模型，其核心思想是将关系看作头实体到尾实体的翻译。对于每个三元组，存在关系式：“头实体向量+关系向量=尾实体向量”

知识图谱嵌入 (3)
TransH（距离模型）

为了解决TransE模型在处理复杂关系时的局限性，TransH模型通过将头实体向量和尾实体向量投影到关系所在的超平面：


知识图谱嵌入 (4)
TransR（距离模型）




知识图谱嵌入 (5)
RESCAL（双线性模型）

将整个KG编码为一个三维张量，由这个张量分解出核心张量和因子矩阵，核心张量中的每个二维矩阵切片代表一种关系，因子矩阵中的每一行代表一个实体


提纲
引例
知识图谱概述
知识图谱构建
知识图谱嵌入
知识图谱推理
总结


知识图谱推理 (1)
核心任务
      旨在基于KG中已有的事实或关系推断未知的事实或关系。
主流方法
基于演绎的推理


基于归纳的推理
      通过已观察到的部分得出一般结论
通过给定一个或多个前提得到一个必然成立的结论
主要有Datalog和产生式规则等方法
基于表示学习、基于规则学习、基于神经网络
传统演绎推理难以适用于大规模KG

知识图谱推理 (2)
通常一般规则包含各种不同的规则类型、表达能力最强，Horn规则次之，路径规则的表达能力最弱

知识图谱推理 (3)
AMIE算法




知识图谱推理 (4)
AMIE算法




知识图谱推理 (5)
AMIE算法



增加悬挂原子（Adding Dandling Atom）。在规则中增加一个原子，包含一个新的变量和一个已在规则中出现的元素，元素可以是出现过的变量或实体
增加实例化原子（Adding Instantiated Atom）。在规则中增加一个原子，包含一个实例化的实体和一个已在规则中出现的元素
增加闭合原子（Adding Closing Atom）。在规则中增加一个原子，包含的两个元素都是已出现在规则中的变量或实体
定义3个挖掘算子：

知识图谱推理 (6)
AMIE算法



引入2个剪枝策略以缩小搜索空间：

知识图谱推理 (7)
AMIE算法



AMIE算法维护一个规则队列，最初包含所有可能的头原子，即所有长度（原子数）为1的规则，然后以迭代的方式将规则出队
若规则是闭合的且满足最小支持度和最小置信度，则输出该规则
若规则长度未超过最大长度阈值，则通过三个挖掘算子扩展该规则（父规则）以生成一组新规则（子规则）
若这些新规则既不重复，也没有根据头覆盖度阈值而被剪枝，则该规则入队
重复该过程，直至队列为空
基本思想：

知识图谱推理 (8)
基于神经网络的推理
基于神经网络的推理方法通过将实体和关系映射到一个非线性隐藏层，具有更强的学习能力、推理能力和泛化能力
 代表方法
   基于神经张量网络（Neural Tensor Network, NTN）的方法：
         NTN将KG中的每个实体嵌入到低维向量空间，捕获实体深层次特征，从           
         而推断实体间存在某种关系的可能性
   基于图神经网络（Graph Neural Network, GNN）的方法：
         GNN主要用于处理图结构数据，基于GNN的推理方法通过同时考虑KG
         的语义信息和结构信息，在大规模KG中丰富实体和关系的表示，并基于
         图进行推理

知识图谱推理 (9)
基于NTN的推理




知识图谱推理 (10)
基于R-GCN的推理（以R-GCN为GNN的代表）




R-GCN首先对每个实体进行编码，每层实体特征由上一层自身实体特征和邻居实体特征加权求和得到
对KG中不同关系连接的实体进行编码表示
考虑自环（Self Loop）以保留实体自身的信息

知识图谱推理 (11)
基于R-GCN的推理（以R-GCN为GNN的代表）





知识图谱推理 (12)
基于R-GCN的推理（以R-GCN为GNN的代表）





R-GCN由N层编码器对KG中的实体进行编码
然后通过评分函数对三元组进行打分
根据得分高低判断三元组是否符合要求

知识图谱推理 (13)
基于R-GCN的推理（以R-GCN为GNN的代表）





提纲
引例
知识图谱概述
知识图谱构建
知识图谱嵌入
知识图谱推理
总结

总结
知识图谱的应用、任务和示例
知识图谱构建的核心任务： 
命名实体识别
关系抽取
实体关系联合抽取
知识图谱嵌入的主要模型分类
距离模型
双线性模型
知识图谱推理的主流方法
基于规则学习的方法
基于神经网络的方法
结语


谢谢！

=== 第9章_贝叶斯网.pptx ===
第9章 贝叶斯网
《智能数据工程》

清华大学出版社
2025年1月
提纲
引例
贝叶斯网概念
贝叶斯网参数学习
贝叶斯网结构学习
基于贝叶斯网的概率推理
总结
引例
哪些事件可能导致患者呼吸困难？
长期吸烟？
感染COVID-19？
 P(呼吸困难=T) =0.01
 P(呼吸困难=T|长期吸烟=T) =0.6
 P(呼吸困难=T|长期吸烟=T, 感染COVID-19=T) =0.9
已知患者长期吸烟
已知患者长期吸烟，且感染了COVID-19
患者无不良生活习惯

遗传、长期吸烟、长期接触致癌物、
感染COVID-19、其他因素…
产生呼吸困难(T)
无呼吸困难症状(F)
患者

知识不完整
信息来源不准确
测试手段的局限性
……
现实世界的推理存在不确定性

引例
概率推理（Probabilistic Inference）基于概率论描述随机事件
     带来的不确定性
常用模型：
贝叶斯网 (Bayesian Network)
马尔科夫网 (Markov Network)
条件随机场 (Conditional Random Field)
概率图模型（Probabilistic Graphical Model）
基于专家经验
推理的局限性
数据和知识来源多 不同领域知识学习成本高昂
实际问题日益复杂 用户需求提高

步骤：
将随机事件视为变量
构建变量间复杂依赖关系
设计概率推理算法
应用场景：金融分析、故障检测、医疗诊断···
提纲
引例
贝叶斯网概念
贝叶斯网参数学习
贝叶斯网结构学习
基于贝叶斯网的概率推理
总结
贝叶斯网（Bayesian Network, BN）
贝叶斯网的基本概念 (1)
有向无环图（Directed Acyclic Graph, DAG）
条件概率表（Conditional Probability Table, CPT）
BN定义
简单的贝叶斯网实例
贝叶斯网的基本概念 (2)
S： “吸烟”
A ：“发烧”
B ：“呼吸困难”
L ：“肺癌”
C ：“感染COVID-19”
所有变量为二值变量（取值为T和F）
变量含义：
贝叶斯网的构建方法
贝叶斯网的基本概念 (3)
手工
构建
贝叶斯网的应用
故障诊断
因果推断
机器学习
推荐系统
通过数据分
析构建BN
模型可解释性高
需要领域专家知识
     工作量大
通过算法学习与数据尽可能吻合的模型
模型构建工作量小
模型构建速度相对快
得到的结构可能需要专家进一步优化


提纲
引例
贝叶斯网概念
贝叶斯网参数学习
贝叶斯网结构学习
基于贝叶斯网的概率推理
总结
BN参数学习
参数学习 (1)
基于样本数据计算变量节点的条件概率参数
BN的参数学习
参数学习 (2)
最大似然估计（Maximum Likelihood Estimation）
参数学习 (3)
步骤：
最大似然估计（Maximum Likelihood Estimation）
参数学习 (4)
其中，P(da|𝜃)为给定𝜃时样本da出现的概率，记为：
最大似然估计（Maximum Likelihood Estimation）
参数学习 (5)
基于最大似然估计的参数学习示例
参数学习 (6)



所有变量为二值变量（取值为T和F）
基于最大似然估计的参数学习实例
参数学习 (7)



基于最大似然估计的参数学习实例
参数学习 (8)



基于最大似然估计的参数学习实例
参数学习 (9)
计算所有节点的CPT：



提纲
引例
贝叶斯网概念
贝叶斯网参数学习
贝叶斯网结构学习
基于贝叶斯网的概率推理
总结
BN的结构学习
结构学习 (1)
在给定数据集的前提下寻找一个与训练样本集匹配最好的网络结构
结构学习主要包括以下两类方法：
卡方测试
（Chi-Square Test）
条件互信息测试（Conditional Mutual Information Test）

条件独立测试
（Conditional Independence Test）
将BN视为描述变量之间条件
独立性关系的网络模型
贝叶斯信息准则
（Bayesian Information Criterion, BIC）
＋
爬山算法

评分搜索
（Scoring and Search）
将BN结构学习视为组合优化问题
基于BIC评分和爬山法的BN结构学习
结构学习 (2)
BIC评分：在大样本前提下对边缘似然函数的一种近似
增加第二项作为惩罚项（Penalty），防止模型过拟合
基于BIC评分和爬山法的BN结构学习
结构学习 (3)
BIC评分的分解：用于减小搜索过程中的计算开销
基于BIC评分和爬山法的BN结构学习
结构学习 (4)
基于爬山法找到BIC评分最高的模型
步骤：
（1）初始结构为无边模型，或基于领域知识设置初始结构
（2）通过加边、减边、反转边三种算子对当前结构局部进行修改，
          得到一系列候选模型
（3）计算不同候选模型参数的最大似然估计及相应的BIC评分
（4）迭代选出当前BIC评分最高的候选结构，直至收敛
基于BIC评分和爬山法的BN结构学习
结构学习 (5)
初始结构
三种算子：
基于爬山法的BN结构学习算法
结构学习 (6)
提纲
引例
贝叶斯网概念
贝叶斯网参数学习
贝叶斯网结构学习
基于贝叶斯网的概率推理
总结
基于BN的概率推理算法
基于贝叶斯网的概率推理 (1)
精确推理算法
已知Ę取值为e的条件下，利用联合概率和边缘概率来计算查询变量Q取值为ɋ的后验概率，得到精确的P(Q=ɋ|Ę=e)
近似推理算法
通过降低对精度的要求，在限定时间内得到一个近似解
- 重要性采样（Importance Sampling）
- 马尔科夫链蒙特卡洛（Markov Chain Monte Carlo, MCMC）
Ę：证据变量集合；Q：查询变量集合
基于BN的精确推理算法
基于贝叶斯网的概率推理 (2)
一般联合概率分布推理
步骤：
整个联合概率分布包含251个独立参数，方法具有极高的复杂度


基于贝叶斯网的概率推理 (3)
利用变量间的条件独立性，分解联合概率分布
需要60次乘法和30次加法
基于贝叶斯网的概率推理 (4)
利用变量间的条件独立性，分解联合概率分布
仅需要20次乘法和10次加法
VE算法
基于贝叶斯网的概率推理 (5)
变量消元法（Variable Elimination, VE）：通过分解联合分布简化推理
步骤：
VE算法
基于贝叶斯网的概率推理 (6)
（2）消元（Elimination）：
VE算法
基于贝叶斯网的概率推理 (7)
Ɓ：BN；Ę：证据变量；e：证据变量取值，Q：查询变量；          ρ：待消元变量顺序，包括所有不在Ę∪Q中的变量

VE算法示例
基于贝叶斯网的概率推理 (8)
基于VE算法计算P(S|L=F)
VE算法示例
基于贝叶斯网的概率推理 (9)
近似推理算法
基于贝叶斯网的概率推理 (10)
Gibbs采样：随机产生一个与证据Ę=e一致的样本Ɗ1作为初始样本，
每一步都从当前样本出发产生下一个样本
步骤：
基于贝叶斯网的概率推理 (11)
基于贝叶斯网的概率推理 (12)
基于Gibbs采样算法近似计算P(S|L=F)
基于Gibbs采样的概率推理示例
随机生成一个与证据L=F一致的样本，假设为Ɗ1={S=T, A=F, B= T, L=F, C=F}
由Ɗ1生成样本Ɗ2
算法从Ɗ2=Ɗ1={S=T, A=F, B=T, L=F}出发，对非证据变量逐个采样
采样顺序为<S, A, B, L, C>
基于贝叶斯网的概率推理 (13)
基于Gibbs采样的概率推理示例
采样过程如下：
① 对S进行采样，mb(S)包含节点A和B，计算S的概率分布P(S|A=F, B=T)，假设采样结果为S=F，则有Ɗ2={S=F, A=F, B=T, L=F, C=F}
② 对A进行采样，此时S=F，mb(A)包含节点S和B，计算F的概率分布P(A|S =F, B=T)。假设采样结果为A=T，则有Ɗ2={S=F, A=T, B=T, L=F, C=F}
基于贝叶斯网的概率推理 (14)
基于Gibbs采样的概率推理示例
③ 对B进行采样，此时A=T，mb(B)包含节点S、A、L和C，因此，计算B的概率分布P(B|S=F, A=T, L=F, C=F)，假设采样结果为B=T，则有Ɗ2={S=F, A= T, B=T, L=F, C=F}
④ 对L进行采样，此时B=T，mb(L)包含节点B和C，计算L的概率分布P(L| B=T, C=F)，假设采样结果为L=T，则有Ɗ2={S=F, A=T, B=T, L=T, C=F}
基于贝叶斯网的概率推理 (15)
基于Gibbs采样的概率推理示例
⑤ 对C进行采样，此时L=T，mb(C)包含节点B和L，计算C的概率分布P(C| B=T, L=T)，假设采样结果为C=F，则有Ɗ2={S=F, A=T, B=y, L=T, C=F}，即为Ɗ2的最终值
提纲
引例
贝叶斯网概念
贝叶斯网参数学习
贝叶斯网结构学习
贝叶斯网概率推理
总结
总结
不确定性知识表示和处理能力，是智能系统走向实用的重要要求
  概率图模型：图模型的概率性质，概率论+图论
  概率推理基本思想和分类
贝叶斯网的概念、参数学习和结构学习
基于贝叶斯网的概率推理（精确推理，近似推理）
结语


谢谢！

